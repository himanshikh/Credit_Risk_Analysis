# -*- coding: utf-8 -*-
"""Credit Risk Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NPBMVPUpIuom5OOpf9aFHx8v5729JJ34
"""

# Step 0: Install imbalanced-learn if not available
!pip install imbalanced-learn

"""# Credit Risk Analysis (Give Me Some Credit Dataset)

## Objective
Predict whether a borrower will **default on a loan within 2 years** using ML.  

**Target:** `SeriousDlqin2yrs`  
- `1` = Default (High risk)  
- `0` = No Default (Low risk)  

---

## Challenges
- Dataset is **imbalanced** (most people repay, few default).  
- Accuracy alone is misleading → use **Balanced Accuracy & Recall**.  

---

## Approach
1. Upload & explore dataset.  
2. Preprocess data (handle missing values, scaling).  
3. Train models:
   - Logistic Regression (baseline)  
   - Logistic Regression + SMOTE (oversampling)  
   - Balanced Random Forest  
   - Easy Ensemble Classifier  
4. Compare results.  

"""

# Step 1: Imports
import pandas as pd
from collections import Counter

# Preprocessing & Models
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from imblearn.over_sampling import RandomOverSampler, SMOTE
from imblearn.combine import SMOTEENN

# Advanced Models
from imblearn.ensemble import BalancedRandomForestClassifier, EasyEnsembleClassifier

# Metrics
from sklearn.metrics import balanced_accuracy_score, confusion_matrix, classification_report
from imblearn.metrics import classification_report_imbalanced

# STEP 2: Upload Dataset
from google.colab import files
uploaded = files.upload()

# STEP 3: Load Dataset
import pandas as pd

df = pd.read_csv("cs-training.csv")
df.head()

"""## Dataset Information
Columns:
- `SeriousDlqin2yrs` → Target (default within 2 years)  
- `RevolvingUtilizationOfUnsecuredLines` → Credit utilization ratio  
- `age` → Borrower’s age  
- `NumberOfTime30-59DaysPastDueNotWorse` → Late payments (30–59 days)  
- `DebtRatio` → Debt / income ratio  
- `MonthlyIncome` → Borrower’s income  
- `NumberOfOpenCreditLinesAndLoans` → Open accounts  
- `NumberOfTimes90DaysLate` → 90+ days late payments  
- `NumberRealEstateLoansOrLines` → Real estate accounts  
- `NumberOfTime60-89DaysPastDueNotWorse` → Late payments (60–89 days)  
- `NumberOfDependents` → Dependents  

"""

# STEP 4: Preprocess Data
# Handle missing values
df = df.dropna()

# Split features and target
X = df.drop("SeriousDlqin2yrs", axis=1)
y = df["SeriousDlqin2yrs"]

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

X_train, X_test, y_train, y_test = train_test_split(
    X, y, stratify=y, test_size=0.2, random_state=42
)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

X_train.shape, X_test.shape

"""## Step 5: Logistic Regression (Baseline)
We start with Logistic Regression on the imbalanced dataset.  
This gives us a **baseline model** to compare with.

"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import balanced_accuracy_score, classification_report

log_reg = LogisticRegression(max_iter=1000)
log_reg.fit(X_train_scaled, y_train)
y_pred_lr = log_reg.predict(X_test_scaled)

print("Balanced Accuracy (Logistic Regression):", balanced_accuracy_score(y_test, y_pred_lr))
print(classification_report(y_test, y_pred_lr))

"""## Step 6: Logistic Regression + SMOTE
To handle imbalance, we apply **SMOTE (Synthetic Minority Oversampling)**.  
This balances the training data by generating synthetic defaults.

"""

from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=42)
X_res, y_res = smote.fit_resample(X_train_scaled, y_train)

log_reg_sm = LogisticRegression(max_iter=1000)
log_reg_sm.fit(X_res, y_res)
y_pred_sm = log_reg_sm.predict(X_test_scaled)

print("Balanced Accuracy (Logistic + SMOTE):", balanced_accuracy_score(y_test, y_pred_sm))

"""## Step 7: Ensemble Models
Instead of resampling, we try models designed for imbalanced data:
- **Balanced Random Forest** → balances classes during training.  
- **Easy Ensemble Classifier** → trains multiple AdaBoost classifiers on balanced subsets.

"""

from imblearn.ensemble import BalancedRandomForestClassifier, EasyEnsembleClassifier

# Balanced Random Forest
brf = BalancedRandomForestClassifier(n_estimators=100, random_state=42)
brf.fit(X_train, y_train)
y_pred_brf = brf.predict(X_test)

# Easy Ensemble Classifier
eec = EasyEnsembleClassifier(n_estimators=100, random_state=42)
eec.fit(X_train, y_train)
y_pred_eec = eec.predict(X_test)

print("Balanced Accuracy (Balanced RF):", balanced_accuracy_score(y_test, y_pred_brf))
print("Balanced Accuracy (Easy Ensemble):", balanced_accuracy_score(y_test, y_pred_eec))

"""## Confusion Matrix & Imbalanced Classification Report

The confusion matrix shows **how many defaults (high-risk)** and **non-defaults (low-risk)** were correctly or incorrectly classified:

- **Rows = Actual values** (what really happened)  
- **Columns = Predicted values** (what the model guessed)  

For example:
- Top-left = Correctly predicted high-risk (True Positives).  
- Top-right = High-risk predicted as low-risk (False Negatives).  
- Bottom-left = Low-risk predicted as high-risk (False Positives).  
- Bottom-right = Correctly predicted low-risk (True Negatives).  

---

The **imbalanced classification report** goes beyond accuracy and includes:  

- **Precision (pre):** Out of all predicted positives, how many were correct?  
- **Recall (rec):** Out of all actual positives, how many did the model catch?  
- **Specificity (spe):** Out of all negatives, how many were identified correctly?  
- **F1 score:** Balance between precision and recall.  
- **Geo mean (geo):** Balance between sensitivity and specificity.  
- **IBA (Index of Balanced Accuracy):** Adjusted measure for imbalanced data.  
- **sup:** Support, i.e., number of samples in each class.  

This helps evaluate **how well the model identifies high-risk borrowers**, which is critical for banking applications.  

"""

from sklearn.metrics import confusion_matrix
from imblearn.metrics import classification_report_imbalanced

# ---- For Balanced Random Forest ----
y_pred_brf = brf.predict(X_test)

matrix_brf = confusion_matrix(y_test, y_pred_brf)
cm_df_brf = pd.DataFrame(
    matrix_brf,
    index=["Actual High-Risk", "Actual Low-Risk"],
    columns=["Predicted High-Risk", "Predicted Low-Risk"]
)

print("Confusion Matrix - Balanced RF")
print(cm_df_brf, "\n")

print("Classification Report - Balanced RF")
print(classification_report_imbalanced(y_test, y_pred_brf))

# ---- For Easy Ensemble ----
y_pred_eec = eec.predict(X_test)

matrix_eec = confusion_matrix(y_test, y_pred_eec)
cm_df_eec = pd.DataFrame(
    matrix_eec,
    index=["Actual High-Risk", "Actual Low-Risk"],
    columns=["Predicted High-Risk", "Predicted Low-Risk"]
)

print("Confusion Matrix - Easy Ensemble")
print(cm_df_eec, "\n")

print("Classification Report - Easy Ensemble")
print(classification_report_imbalanced(y_test, y_pred_eec))

"""# Conclusion

- Both models outperform the **baseline Logistic Regression** on imbalanced credit risk data.  
- **Balanced Random Forest** achieves **higher precision** but misses more defaulters (recall ~65%).  
- **Easy Ensemble Classifier** achieves **higher recall (~73%)**, meaning it catches more high-risk borrowers — crucial for banking risk management.  
- Trade-off: Higher recall comes at the cost of more false alarms (flagging some safe customers as risky).  

### Key Business Insight
In banking, **missing a high-risk borrower (false negative)** is far more costly than flagging a safe one.  
Therefore, the **Easy Ensemble Classifier is the preferred model**, as it minimizes default risk by ensuring most defaulters are detected.  

Final Outcome:  
This project demonstrates how **ensemble learning methods significantly improve loan default prediction** compared to simple classifiers, and provides a framework banks like ICICI can use to **reduce financial risk**.

"""

